---
title: "Machine Learning with Python"
author: "Marshall McQuillen"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# EDA Recap

After getting to know the data during [EDA](eda_with_R.pdf), some general statements can be made with regard to survival status:

* A passenger's probability of surviving decreases as their socioeconomic status decreases.
* Given a passenger is male, he almost certainly died.
* Given a passenger is female, she most likely survived.
* Children had a higher probability of surviving than the elderly.
* A passenger's probability of surviving decreases as the number of siblings (or spouse) they have on board increases.

# Machine Learning Outline

Using some of the insights (summarized above) that were extracted from the exploratory analysis, this part of the analysis will shift to machine learning, with the overarching goal being to correctly classify those who lived and those who died aboard the Titanic.

This section will be broken down into four parts:

1. **Data Cleaning** - As mentioned in the [Exploratory Analysis](eda_with_R.pdf), there are some Null and NaN value that needs to be taken care of.
2. **Create a Benchmark** - Logistic Regression with a few select coefficients.
3. **Multi-Model Testing** - Cross validate a variety of classification algorithms, performing minor grid searches over each.
4. **Algorithm Tuning and Explanation** - Choose the model with the highest accuracy, perform a more granular grid search and explain the mathematical foundations of the algorithm.
5. **Test Set Scoring** - Use the refined model on the test set.

## Data Cleaning

First and Foremost, as mentioned briefly in the exploratory analysis with R, there is a small amount of data cleaning that needs to be performed before modeling. As it turns out, over 75% of the values in the Cabin column are empty strings. Initially, one might try grouping on the SipSp and Parch columns and then looking at the last names to determine any families that were traveling together. Hypothetically, this would then lead to some of the family members having entries in the Cabin column, and one could then impute the values from there. Unfortunately, when this was performed the theoretical family groupings either all had no entries in the Cabin column or they were all complete. This, paired with the fact that imputing over 75% of the values for a column could only be considered guesswork at best, leads to the decision to not include this column in the modeling process.

In addition, there are 177 NaN values in the Age column, which amounts to roughly 20% of the data set. Since this variable *will* be used in the modeling process, some imputation will be necessary. Instead of imputing the **entire** column mean for **all** of the missing values, the distortion to the distribution of the values can be minimized by getting a little more granular.

In order to do this, any NaN values in the Age column were imputed with the mean of a *subset* of the data. For each of the possible combinations of the distinct values in the Gender, Pclass and Title (a feature constructed out of the Name column, which appears to be a rough proxy for Age) columns, a subset was created by  masking the entire data set to only include observations in *that specific group,* and then the mean **of that subset** was imputed was for the NaN values **in that subset.** Doing this for all of the possible subsets results in the 177 NaN being imputed in an intelligent manner.

Lastly, there were two observations that didn't have values in the Embarked column, which were droppped since there will be close to no affect going from 891 observations to 889 observations. In addition, if there hypothetically were an improvement in the error rate, it is unlikely that it would have been worth the time spent to work through an intelligent manner to impute those values.


### Benchmark Model - Logistic Regression

The model that will be used as the benchmark will be logistic regression using the Age, Gender, Parch and Sibsp variables, as well as all the combinations of those variables, allowing for interaction effects to occur.

$$\hat{y}~=~$$

